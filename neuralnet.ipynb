{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM69xdNvK0iOM8B5UQ3iJJt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WillRWhite/Colab/blob/main/neuralnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8CXUO6WeszX"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------------------ #\n",
        "#                                                                                #\n",
        "#                     Neural Network Example                                     #\n",
        "#                     ----------------------                                     #\n",
        "#                                                                                #\n",
        "# This is a work in progress.                                                    #\n",
        "# The class Netlayer is a template for a neural network single layer             #\n",
        "# Its parameters are the inputs as a list and the length of the outputs          #\n",
        "# from the layer. The __init__ method initilises the weights and biass for the   #\n",
        "# outputs. The forward method takes the activation function as a parameter.      #\n",
        "# Currently the activation functions (Sigmois & RelU ) need compleating.         #\n",
        "# Currently they do nothing. ALso a back propogation function needs to be        #\n",
        "# implemented.                                                                   #\n",
        "#                                                                                #\n",
        "# ------------------------------------------------------------------------------ #\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# The \"main()\" entry point\n",
        "def main():\n",
        "\n",
        "  # Some input samples:\n",
        "  #inputs = np.array([1,2,3,4])\n",
        "  #inputs = np.array([1,2,3,4,5,6,7, 8, 9])\n",
        "  inputs = np.random.randint(0,255,784)\n",
        "  #print(\"Inputs=\", inputs)\n",
        "\n",
        "  layer1 = NetLayer(inputs, 16)\n",
        "  layer1_out = layer1.forward()\n",
        "  print(\"layer1 Outputs =\", layer1_out )\n",
        "\n",
        "  layer2 = NetLayer(layer1_out, 10 )\n",
        "  layer2_out = layer2.forward(\"S\")\n",
        "  print(\"layer2 Outputs =\", layer2_out )\n",
        "\n",
        "  layer3 = NetLayer(layer2_out, 2)\n",
        "  layer3_out = layer3.forward(\"R\")\n",
        "  print(\"layer3 Outputs =\", layer3_out )\n",
        "\n",
        "\n",
        "# Debug print function\n",
        "def debug_print(*objects, d=True):\n",
        "  if d == True:\n",
        "    print(\"DEBUG:- \", end=\"\")\n",
        "    print(*objects)\n",
        "\n",
        "# Single neural net layer template\n",
        "class NetLayer:\n",
        "\n",
        "    # The \"instance_count\" class variable will be incremented in the \"__init__\" method each time the class is instasnciated\n",
        "    # Usefull for debugging. Can be removed in final release\n",
        "    instance_count = 0\n",
        "\n",
        "    def __init__(self, inputs, out_len):\n",
        "        # Increment the instance_count each time NetLayer is instanciated\n",
        "        NetLayer.instance_count += 1\n",
        "\n",
        "        self.inputs = inputs\n",
        "        input_len = np.size(inputs)\n",
        "        debug_print(\"Input size =\", input_len, d=DEBUG)\n",
        "        self.weights = np.random.uniform(-1, 1, (input_len, out_len))\n",
        "        self.bias = np.random.uniform(-1, 1, out_len)\n",
        "        if out_len > input_len:\n",
        "            # Debug code to identify which instance\n",
        "            debug_print(f\"{self.__class__.__name__} has been called by instance{NetLayer.instance_count}\", d=DEBUG)\n",
        "            # For a neural net layer the outputs should normally be less than the inputs\n",
        "            debug_print(\"NetLayer instance\",NetLayer.instance_count,\":- Warning more outputs(\" + str(out_len) + \") then inputs(\" + str(input_len) + \")\",d=DEBUG)\n",
        "        #print(\"Weights =\", self.weights)\n",
        "        debug_print(\"NetLayer instance\",NetLayer.instance_count,\"Weights =\", self.weights, d=DEBUG)\n",
        "\n",
        "    def forward(self, a=\"R\"):\n",
        "        debug_print(\"Running forward()\", d=DEBUG)\n",
        "        outputs = np.matmul(self.inputs, self.weights) + self.bias\n",
        "        # Sigmoid activation\n",
        "        if a == \"S\":\n",
        "            debug_print(\"Sigmoid Activation\", d=DEBUG)\n",
        "            pass\n",
        "        # RelU activation\n",
        "        # \"R\" or any other input\n",
        "        else:\n",
        "            debug_print(\"RelU Activation\",d=DEBUG)\n",
        "            pass\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# Run the main program\n",
        "\n",
        "# Setting DEBUG to True will enable the \"debug_print()\" function\n",
        "DEBUG=True\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()\n"
      ]
    }
  ]
}